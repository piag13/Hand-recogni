{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd03478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edeb5c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'core' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#@markdown We implemented some functions to visualize the hand landmark detection results. <br/> Run the following cell to activate the functions.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmediapipe\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m solutions\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmediapipe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mformats\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m landmark_pb2\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\__init__.py:17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmediapipe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmediapipe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msolutions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msolutions\u001B[39;00m \n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmediapipe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtasks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtasks\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m framework\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m gpu\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m audio\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m components\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m core\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:27\u001B[0m\n\u001B[0;32m     25\u001B[0m AudioEmbedderOptions \u001B[38;5;241m=\u001B[39m audio_embedder\u001B[38;5;241m.\u001B[39mAudioEmbedderOptions\n\u001B[0;32m     26\u001B[0m AudioEmbedderResult \u001B[38;5;241m=\u001B[39m audio_embedder\u001B[38;5;241m.\u001B[39mAudioEmbedderResult\n\u001B[1;32m---> 27\u001B[0m RunningMode \u001B[38;5;241m=\u001B[39m \u001B[43mcore\u001B[49m\u001B[38;5;241m.\u001B[39maudio_task_running_mode\u001B[38;5;241m.\u001B[39mAudioTaskRunningMode\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Remove unnecessary modules to avoid duplication in API docs.\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m audio_classifier\n",
      "\u001B[1;31mNameError\u001B[0m: name 'core' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Squential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51897c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.putText(frame, f'frame', (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hand landmark mediapipe",
   "id": "2ac402365c5a449c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c88527",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001B[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001B[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001B[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print handedness and draw hand landmarks on the image.\n",
    "    print('Handedness:', results.multi_handedness)\n",
    "    if not results.multi_hand_landmarks:\n",
    "      continue\n",
    "    image_height, image_width, _ = image.shape\n",
    "    annotated_image = image.copy()\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "      print('hand_landmarks:', hand_landmarks)\n",
    "      print(\n",
    "          f'Index finger tip coordinates: (',\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "      )\n",
    "      mp_drawing.draw_landmarks(\n",
    "          annotated_image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "    cv2.imwrite(\n",
    "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
    "    # Draw hand world landmarks.\n",
    "    if not results.multi_hand_world_landmarks:\n",
    "      continue\n",
    "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "      mp_drawing.plot_landmarks(\n",
    "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd4f8e",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb7315db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T03:04:04.161997Z",
     "start_time": "2025-05-12T03:03:59.781447Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = 'data/processed_combine_asl_dataset'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(dataset_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        for img_file in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            images.append(img)\n",
    "            labels.append(1 if label == 'palm' else 0)\n",
    "\n",
    "    images = np.array(images) / 255.0\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_dataset(DATASET_PATH)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display some training images\n",
    "unique_labels = np.unique(y_train)\n",
    "plt.figure(figsize=(10, 5))\n",
    "for label in unique_labels:\n",
    "    index = np.where(y_train == label)[0][0]\n",
    "    plt.subplot(1, len(unique_labels), label + 1)\n",
    "    plt.imshow(X_train[index])\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Focal loss\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)\n",
    "                      + (1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss=focal_loss(gamma=2., alpha=.25), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save model\n",
    "model.save('palm_detector_cnn.h5')\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "461e7ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T03:14:45.582989Z",
     "start_time": "2025-05-12T03:14:09.210823Z"
    }
   },
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_hand_landmarks(image, hands):\n",
    "    \"\"\"Extract hand landmarks from an image using MediaPipe\"\"\"\n",
    "    # Convert the BGR image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and detect hands\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    # Initialize landmarks array\n",
    "    landmarks = np.zeros((21, 3))  # 21 landmarks, each with x, y, z coordinates\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        # Get the first hand detected\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Draw landmarks on the image (for visualization)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Extract landmark coordinates\n",
    "        for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "            landmarks[idx] = [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "    return landmarks.flatten(), image  # Return both landmarks and annotated image\n",
    "\n",
    "def load_and_preprocess_dataset(dataset_path):\n",
    "    \"\"\"Load and preprocess images to hand landmarks\"\"\"\n",
    "    X_landmarks = []\n",
    "    labels = []\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {dataset_path}\")\n",
    "\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.5) as hands:\n",
    "\n",
    "        for label in os.listdir(dataset_path):\n",
    "            label_path = os.path.join(dataset_path, label)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing {label} images...\")\n",
    "            for img_file in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_file)\n",
    "                # Read image\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Warning: Could not read image {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract landmarks\n",
    "                landmarks, _ = extract_hand_landmarks(image, hands)\n",
    "\n",
    "                # Add to dataset if hand was detected (non-zero landmarks)\n",
    "                if np.any(landmarks):\n",
    "                    X_landmarks.append(landmarks)\n",
    "                    labels.append(1 if label == 'palm' else 0)\n",
    "\n",
    "    if not X_landmarks:\n",
    "        raise ValueError(\"No valid hand landmarks were detected in the dataset\")\n",
    "\n",
    "    return np.array(X_landmarks), np.array(labels)\n",
    "\n",
    "def create_landmark_model(input_shape):\n",
    "    \"\"\"Create model for landmark-based classification\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def test_real_time():\n",
    "    \"\"\"Real-time testing with webcam\"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "\n",
    "    with mp_hands.Hands(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame\")\n",
    "                break\n",
    "\n",
    "            # Extract landmarks and get annotated frame\n",
    "            landmarks, annotated_frame = extract_hand_landmarks(frame, hands)\n",
    "\n",
    "            # Make prediction if hand is detected\n",
    "            if np.any(landmarks):\n",
    "                prediction = model.predict(landmarks.reshape(1, -1), verbose=0)[0]\n",
    "                label = \"Palm\" if prediction > 0.5 else \"Not Palm\"\n",
    "                confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "\n",
    "                # Draw prediction\n",
    "                cv2.putText(annotated_frame, f\"{label}: {confidence:.2f}\",\n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                           (0, 255, 0), 2)\n",
    "\n",
    "            # Display frame\n",
    "            cv2.imshow('Hand Landmark Detection', annotated_frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Training parameters\n",
    "    DATASET_PATH = 'data/processed_combine_asl_dataset'\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    try:\n",
    "        # Load and preprocess dataset\n",
    "        print(\"Loading and preprocessing dataset...\")\n",
    "        X, y = load_and_preprocess_dataset(DATASET_PATH)\n",
    "\n",
    "        # Split dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create and train model\n",
    "        input_shape = X_train.shape[1]\n",
    "        model = create_landmark_model(input_shape)\n",
    "\n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        model.save('hand_landmark_detector.h5')\n",
    "        print(\"Model saved as 'hand_landmark_detector.h5'\")\n",
    "\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Run real-time testing\n",
    "        print(\"Starting real-time testing... Press 'q' to quit\")\n",
    "        test_real_time()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing dataset...\n",
      "Processing 0 images...\n",
      "Processing 1 images...\n",
      "Processing 2 images...\n",
      "Processing 3 images...\n",
      "Processing 4 images...\n",
      "Processing 5 images...\n",
      "Processing 6 images...\n",
      "Processing 7 images...\n",
      "Processing 8 images...\n",
      "Processing 9 images...\n",
      "Processing a images...\n",
      "Error: No valid hand landmarks were detected in the dataset\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
